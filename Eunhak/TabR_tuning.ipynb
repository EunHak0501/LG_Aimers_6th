{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# conda install -c conda-forge faiss-gpu\n",
    "\n",
    "# conda 가상환경 상에서 설치 (로컬로 돌릴때)"
   ],
   "id": "7df326ea119bcc41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 현재 작업 디렉토리(Eunhak)에서 tabular_dl_tabr 경로 추가\n",
    "project_path = os.path.join(os.getcwd(), \"tabular_dl_tabr\")\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "\n",
    "project_dir = Path(r\"C:\\workspace\\LG_Aimers_6th\\Eunhak\\tabular_dl_tabr\")\n",
    "os.environ['PROJECT_DIR'] = str(project_dir)\n",
    "\n",
    "# 경로가 존재하지 않으면 생성\n",
    "if not project_dir.exists():\n",
    "    project_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import delu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from bin.tabr import Model\n",
    "from LG_Aimers_6th.cal_auc import calculate_auc"
   ],
   "id": "2910d27905e82153"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_seed = 1\n",
    "\n",
    "train_path = f'../data/custom_train_{data_seed}.csv'\n",
    "test_path = f'../data/custom_test_{data_seed}.csv'\n",
    "\n",
    "# 학습/평가 데이터 로드\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID']) # test에는 target이 없음\n",
    "\n",
    "print(train.shape, test.shape)"
   ],
   "id": "187601e65a5eece5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from preprocess_DL import all_process\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "print(train.shape, test.shape)"
   ],
   "id": "2137a6bb27553804"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_cols(df_train, target_col='임신 성공 여부'):\n",
    "    cat_cols = [col for col in df_train.columns if pd.api.types.is_categorical_dtype(df_train[col])]\n",
    "    numeric_cols = [col for col in df_train.columns if col not in cat_cols and col != '임신 성공 여부']\n",
    "\n",
    "    num_cols = []\n",
    "    bin_cols = []\n",
    "    for col in numeric_cols:\n",
    "        if df_train[col].nunique() == 2:\n",
    "            bin_cols.append(col)\n",
    "        else:\n",
    "            num_cols.append(col)\n",
    "\n",
    "    return num_cols, bin_cols, cat_cols\n",
    "\n",
    "num_cols, bin_cols, cat_cols = get_cols(train)\n",
    "cat_cardinalities = [train[col].nunique() for col in cat_cols]\n",
    "\n",
    "print(f'수치형 변수: {len(num_cols)}개 \\n{num_cols}')\n",
    "print(f'이진형 변수: {len(bin_cols)}개 \\n{bin_cols}')\n",
    "print(f'범주형 변수: {len(cat_cols)}개 \\n{cat_cols}')"
   ],
   "id": "9c7d1b151ba2232f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_dataset_from_dfs(train_df, valid_df, test_df, num_cols, bin_cols, cat_cols, target_col='임신 성공 여부'):\n",
    "    data = {}\n",
    "    data['X_num'] = {\n",
    "        'train': torch.tensor(train_df[num_cols].values, dtype=torch.float32),\n",
    "        'val':   torch.tensor(valid_df[num_cols].values, dtype=torch.float32),\n",
    "        'test':  torch.tensor(test_df[num_cols].values, dtype=torch.float32),\n",
    "    }\n",
    "    data['X_bin'] = {\n",
    "        'train': torch.tensor(train_df[bin_cols].values, dtype=torch.float32),\n",
    "        'val':   torch.tensor(valid_df[bin_cols].values, dtype=torch.float32),\n",
    "        'test':  torch.tensor(test_df[bin_cols].values, dtype=torch.float32),\n",
    "    }\n",
    "    if cat_cols:\n",
    "        data['X_cat'] = {\n",
    "            'train': torch.tensor(train_df[cat_cols].values, dtype=torch.long),\n",
    "            'val':   torch.tensor(valid_df[cat_cols].values, dtype=torch.long),\n",
    "            'test':  torch.tensor(test_df[cat_cols].values, dtype=torch.long),\n",
    "        }\n",
    "    else:\n",
    "        data['X_cat'] = None\n",
    "    data['Y'] = {\n",
    "        'train': torch.tensor(train_df[target_col].values, dtype=torch.long),\n",
    "        'val':   torch.tensor(valid_df[target_col].values, dtype=torch.long),\n",
    "        # test 데이터에는 타깃이 없을 수 있습니다.\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def move_data_to_device(data, device):\n",
    "    # data는 dict 형식: 예) {'X_num': {'train': tensor, 'val': tensor, ...}, ...}\n",
    "    for key in data:\n",
    "        if data[key] is None:\n",
    "            continue\n",
    "        if isinstance(data[key], dict):\n",
    "            for part in data[key]:\n",
    "                data[key][part] = data[key][part].to(device)\n",
    "        else:\n",
    "            data[key] = data[key].to(device)\n",
    "    return data\n",
    "\n",
    "\n",
    "class MyDataset:\n",
    "    def __init__(self, data, n_num_features, n_bin_features, cat_cardinalities, is_regression=False, is_multiclass=True):\n",
    "        self.data = data\n",
    "        self._n_num_features = n_num_features\n",
    "        self._n_bin_features = n_bin_features\n",
    "        self._cat_cardinalities = cat_cardinalities\n",
    "        self.is_regression = is_regression\n",
    "        self.is_multiclass = is_multiclass\n",
    "\n",
    "    @property\n",
    "    def n_num_features(self):\n",
    "        return self._n_num_features\n",
    "\n",
    "    @property\n",
    "    def n_bin_features(self):\n",
    "        return self._n_bin_features\n",
    "\n",
    "    def cat_cardinalities(self):\n",
    "        return self._cat_cardinalities\n",
    "\n",
    "    @property\n",
    "    def Y(self):\n",
    "        return self.data['Y']\n",
    "\n",
    "    def size(self, part: str) -> int:\n",
    "        # target이 있는 경우 사용\n",
    "        if part in self.data['Y']:\n",
    "            return self.data['Y'][part].shape[0]\n",
    "        else:\n",
    "            return self.data['X_num'][part].shape[0]"
   ],
   "id": "513110eea28cb45d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = Model(\n",
    "    n_num_features=len(num_cols),\n",
    "    n_bin_features=len(bin_cols),\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    n_classes=2,\n",
    "    num_embeddings=None,      # 임베딩 사용하지 않을 경우 None\n",
    "    d_main=64,\n",
    "    d_multiplier=2.0,\n",
    "    encoder_n_blocks=2,\n",
    "    predictor_n_blocks=2,\n",
    "    mixer_normalization=True,\n",
    "    context_dropout=0.1,\n",
    "    dropout0=0.1,\n",
    "    dropout1='dropout0',      # 'dropout0' 문자열을 지정하면 내부에서 dropout0 값이 사용됩니다.\n",
    "    normalization=\"BatchNorm1d\",\n",
    "    activation=\"ReLU\",\n",
    "    memory_efficient=False,\n",
    "    candidate_encoding_batch_size=None,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ],
   "id": "421c452120c78e2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "seed = 333\n",
    "\n",
    "def objective(trial):\n",
    "    ### 하이퍼파라미터 탐색 공간 설정 ###\n",
    "\n",
    "    d_main = trial.suggest_int(\"d_main\", 32, 128, step=32)\n",
    "    d_multiplier = trial.suggest_float(\"d_multiplier\", 1.0, 3.0, step=0.5)\n",
    "    encoder_n_blocks = trial.suggest_int(\"encoder_n_blocks\", 1, 3)\n",
    "    predictor_n_blocks = trial.suggest_int(\"predictor_n_blocks\", 1, 3)\n",
    "    dropout0 = trial.suggest_float(\"dropout0\", 0.0, 0.5, step=0.05)\n",
    "    context_size = trial.suggest_int(\"context_size\", 2, 64)\n",
    "    context_dropout = trial.suggest_float(\"context_dropout\", 0.0, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    def get_Xy(part: str, idx: torch.Tensor = None) -> tuple[dict, torch.Tensor]:\n",
    "        batch = (\n",
    "            { key[2:]: dataset.data[key][part] for key in dataset.data if key.startswith('X_') },\n",
    "            dataset.data['Y'][part] if 'Y' in dataset.data and part in dataset.data['Y'] else None\n",
    "        )\n",
    "        if idx is None:\n",
    "            return batch\n",
    "        else:\n",
    "            return (\n",
    "                {k: v[idx] for k, v in batch[0].items()},\n",
    "                batch[1][idx] if batch[1] is not None else None\n",
    "            )\n",
    "\n",
    "    def apply_model(part: str, idx: torch.Tensor, is_train: bool) -> torch.Tensor:\n",
    "        x, y = get_Xy(part, idx)\n",
    "        candidate_indices = train_indices\n",
    "        if is_train:\n",
    "            # training part: 후보에서 현재 배치 제거\n",
    "            candidate_indices = candidate_indices[~torch.isin(candidate_indices, idx)]\n",
    "        # 후보 데이터: 조건에 따라 전체 train 또는 선택된 인덱스 사용\n",
    "        candidate_x, candidate_y = get_Xy('train', None if candidate_indices.equal(train_indices) else candidate_indices)\n",
    "        return model(\n",
    "            x_=x,\n",
    "            y=y if is_train else None,\n",
    "            candidate_x_=candidate_x,\n",
    "            candidate_y=candidate_y,\n",
    "            context_size=context_size,\n",
    "            is_train=is_train,\n",
    "        ).squeeze(-1)\n",
    "\n",
    "\n",
    "    valid_aucs = []\n",
    "    test_aucs = []\n",
    "\n",
    "    data_seeds = [1, 7]\n",
    "    for data_seed in data_seeds:\n",
    "        valid_scores = []\n",
    "        test_preds = []\n",
    "\n",
    "        torch.manual_seed(333)\n",
    "        delu.random.seed(seed)\n",
    "        np.random.seed(333)\n",
    "        random.seed(333)\n",
    "\n",
    "        train_path = f'../data/custom_train_{data_seed}.csv'\n",
    "        test_path = f'../data/custom_test_{data_seed}.csv'\n",
    "\n",
    "        train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "        test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['임신 성공 여부'])):\n",
    "            fold_train = train.iloc[train_idx].copy().reset_index(drop=True)\n",
    "            fold_valid = train.iloc[valid_idx].copy().reset_index(drop=True)\n",
    "            fold_train2 = fold_train.copy()\n",
    "            fold_test = test.copy()\n",
    "\n",
    "            fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "            _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "            num_cols, bin_cols, cat_cols = get_cols(fold_train)\n",
    "            cat_cardinalities = [fold_train[col].nunique() for col in cat_cols]\n",
    "\n",
    "            data_dict = build_dataset_from_dfs(\n",
    "                fold_train, fold_valid, fold_test,\n",
    "                num_cols, bin_cols, cat_cols, target_col='임신 성공 여부'\n",
    "            )\n",
    "            data_dict = move_data_to_device(data_dict, device)\n",
    "            dataset = MyDataset(data_dict, n_num_features=len(num_cols), n_bin_features=len(bin_cols), cat_cardinalities=cat_cardinalities)\n",
    "\n",
    "            train_size = dataset.size('train')\n",
    "            train_indices = torch.arange(train_size, device=device)\n",
    "\n",
    "            model = Model(\n",
    "                n_num_features=len(num_cols),\n",
    "                n_bin_features=len(bin_cols),\n",
    "                cat_cardinalities=cat_cardinalities,\n",
    "                n_classes=2,\n",
    "                num_embeddings=None,      # 임베딩 사용하지 않을 경우 None\n",
    "                d_main=d_main,\n",
    "                d_multiplier=d_multiplier,\n",
    "                encoder_n_blocks=encoder_n_blocks,\n",
    "                predictor_n_blocks=predictor_n_blocks,\n",
    "                mixer_normalization=True,\n",
    "                context_dropout=context_dropout,\n",
    "                dropout0=dropout0,\n",
    "                dropout1='dropout0',      # 'dropout0' 문자열을 지정하면 내부에서 dropout0 값이 사용됩니다.\n",
    "                normalization=\"BatchNorm1d\",\n",
    "                activation=\"ReLU\",\n",
    "                memory_efficient=False,\n",
    "                candidate_encoding_batch_size=None,\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            num_epochs = 100000\n",
    "            batch_size = 2048\n",
    "\n",
    "            patience = 10\n",
    "            best_val_loss = float('inf')\n",
    "            early_stop_counter = 0\n",
    "\n",
    "            checkpoint_path = \"best_model_TabR.pth\"\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                # 학습 데이터 인덱스 섞기\n",
    "                shuffled_indices = train_indices[torch.randperm(train_size)]\n",
    "                num_batches = math.ceil(train_size / batch_size)\n",
    "                epoch_loss = 0.0\n",
    "                for i in range(num_batches):\n",
    "                    idx = shuffled_indices[i * batch_size : (i + 1) * batch_size]\n",
    "                    outputs = apply_model('train', idx, is_train=True)\n",
    "\n",
    "                    # 해당 인덱스의 타깃\n",
    "                    _, y_batch = get_Xy('train', idx)\n",
    "\n",
    "                    y_batch = y_batch.float()\n",
    "                    loss = criterion(outputs.squeeze(), y_batch.squeeze()) # squeeze해서 shape 맞추기 (예: (batch_size,))\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item() * idx.numel()\n",
    "\n",
    "                avg_loss = epoch_loss / train_size\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_indices = torch.arange(dataset.size('val'), device=device)\n",
    "                    outputs_val = apply_model('val', val_indices, is_train=False)\n",
    "                    _, y_val = get_Xy('val', val_indices)\n",
    "\n",
    "                    val_loss = criterion(outputs_val.squeeze(), y_val.float().squeeze()).item() # validation loss 계산\n",
    "\n",
    "                    outputs_val = torch.sigmoid(outputs_val)\n",
    "                    outputs_val_np = outputs_val.detach().cpu().numpy().squeeze()\n",
    "                    y_val_np = y_val.detach().cpu().numpy().squeeze()\n",
    "\n",
    "                    val_auc = roc_auc_score(y_val_np, outputs_val_np)\n",
    "\n",
    "                # print(f\"[Epoch {epoch+1}] Train Loss: {avg_loss:.4f}, Valid Loss: {val_loss:.4f}, Valid AUC: {val_auc:.6f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    early_stop_counter = 0\n",
    "                    best = {'epoch': epoch+1, 'val_loss': val_loss, 'val_auc': val_auc}\n",
    "                    torch.save(model.state_dict(), checkpoint_path)\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= patience:\n",
    "                        break\n",
    "\n",
    "            model.load_state_dict(torch.load(checkpoint_path))\n",
    "            # print(f'\\n[Fold{fold+1} Result]')\n",
    "            # print(best)\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_indices = torch.arange(dataset.size('test'), device=device)\n",
    "\n",
    "                outputs_val = torch.sigmoid(apply_model('val', val_indices, is_train=False))\n",
    "                outputs_val_np = outputs_val.detach().cpu().numpy().squeeze()\n",
    "                _, y_val = get_Xy('val', val_indices)\n",
    "                y_val_np = y_val.detach().cpu().numpy().squeeze()\n",
    "\n",
    "                valid_score = roc_auc_score(y_val_np, outputs_val_np)\n",
    "                valid_scores.append(valid_score)\n",
    "\n",
    "                y_pred_test = torch.sigmoid(apply_model('test', test_indices, is_train=False))\n",
    "                y_pred_test_np = y_pred_test.detach().cpu().numpy().squeeze()\n",
    "                test_preds.append(y_pred_test_np)\n",
    "                test_auc_fold = calculate_auc(y_pred_test_np, seed=data_seed)\n",
    "                # print(f'[Data Seed {data_seed} Fold {fold+1}] Valid AUC: {valid_score:.5f}, Test AUC: {test_auc_fold:.5f}')\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        valid_auc = np.mean(valid_scores, axis=0)\n",
    "        valid_aucs.append(valid_auc)\n",
    "        test_auc = calculate_auc(np.mean(test_preds, axis=0), data_seed)\n",
    "        test_aucs.append(test_auc)\n",
    "\n",
    "        print('-' * 60)\n",
    "        print(f'[Data Seed {data_seed}] AVG Valid AUC: {valid_auc:.5f}, Test AUC: {test_auc}')\n",
    "\n",
    "    avg_valid_auc = np.mean(valid_aucs)\n",
    "    avg_test_auc = np.mean(test_aucs)\n",
    "\n",
    "    print('-' * 60)\n",
    "    print(f'[Data Seed 1,7] AVG Valid AUC: {avg_valid_auc:.5f}, Test AUC: {avg_test_auc}')\n",
    "    print('-' * 60)\n",
    "\n",
    "    return avg_test_auc\n",
    "\n",
    "# Optuna 스터디 생성 (최대화 방향)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ],
   "id": "821acd54bf044ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "92760a29e96e1c4c"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
