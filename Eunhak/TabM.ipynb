{
 "cells": [
  {
   "cell_type": "code",
   "id": "b0919eeb-bbb3-48d4-ae3f-3dac6b370d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:54:34.000955Z",
     "start_time": "2025-04-03T23:54:31.890285Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from tabm_reference import Model, make_parameter_groups\n",
    "import rtdl_num_embeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from LG_Aimers_6th.cal_auc import calculate_auc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "cd8b152b-c7a9-40f1-975a-516b764fcadd",
   "metadata": {},
   "source": [
    "## 2. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "id": "feed2e5f-fb44-4ec5-8546-711ee29221cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:54:39.709711Z",
     "start_time": "2025-04-03T23:54:38.780487Z"
    }
   },
   "source": [
    "data_seed = 1\n",
    "\n",
    "train_path = f'../data/custom_train_{data_seed}.csv'\n",
    "test_path = f'../data/custom_test_{data_seed}.csv'\n",
    "\n",
    "# í•™ìŠµ/í‰ê°€ ë°ì´í„° ë¡œë“œ\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "print(train.shape, test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205080, 68) (51271, 67)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "63299de32ba2bec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:54:50.208500Z",
     "start_time": "2025-04-03T23:54:45.817259Z"
    }
   },
   "source": [
    "from preprocess_DL import all_process\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "train, test = all_process(train, test)\n",
    "\n",
    "cat_cols = [col for col in train.columns if pd.api.types.is_categorical_dtype(train[col])]\n",
    "numeric_cols = [col for col in train.columns if col not in cat_cols and col != 'ì„ì‹  ì„±ê³µ ì—¬ë¶€']\n",
    "\n",
    "print(f'ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ \\n{numeric_cols}')\n",
    "print(f'ë²”ì£¼í˜• ë³€ìˆ˜: {len(cat_cols)}ê°œ \\n{cat_cols}')\n",
    "print(train.shape, test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìˆ˜ì¹˜í˜• ë³€ìˆ˜: 57ê°œ \n",
      "['ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜', 'ë°°ë€ ìê·¹ ì—¬ë¶€', 'ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€', 'ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€', 'ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€', 'ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸', 'ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸', 'ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸', 'ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸', 'ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸', 'ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸', 'ë¶ˆëª…í™• ë¶ˆì„ ì›ì¸', 'ë¶ˆì„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜', 'ë¶ˆì„ ì›ì¸ - ë‚¨ì„± ìš”ì¸', 'ë¶ˆì„ ì›ì¸ - ë°°ë€ ì¥ì• ', 'ë¶ˆì„ ì›ì¸ - ìê¶ê²½ë¶€ ë¬¸ì œ', 'ë¶ˆì„ ì›ì¸ - ìê¶ë‚´ë§‰ì¦', 'ë¶ˆì„ ì›ì¸ - ì •ì ë†ë„', 'ë¶ˆì„ ì›ì¸ - ì •ì ìš´ë™ì„±', 'ë¶ˆì„ ì›ì¸ - ì •ì í˜•íƒœ', 'í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜', 'IVF ì‹œìˆ  íšŸìˆ˜', 'DI ì‹œìˆ  íšŸìˆ˜', 'ì´ ì„ì‹  íšŸìˆ˜', 'IVF ì„ì‹  íšŸìˆ˜', 'DI ì„ì‹  íšŸìˆ˜', 'ì´ ì¶œì‚° íšŸìˆ˜', 'IVF ì¶œì‚° íšŸìˆ˜', 'DI ì¶œì‚° íšŸìˆ˜', 'ì´ ìƒì„± ë°°ì•„ ìˆ˜', 'ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜', 'ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜', 'ì´ì‹ëœ ë°°ì•„ ìˆ˜', 'ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜', 'ì €ì¥ëœ ë°°ì•„ ìˆ˜', 'ë¯¸ì„¸ì£¼ì… í›„ ì €ì¥ëœ ë°°ì•„ ìˆ˜', 'í•´ë™ëœ ë°°ì•„ ìˆ˜', 'í•´ë™ ë‚œì ìˆ˜', 'ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œì ìˆ˜', 'ì €ì¥ëœ ì‹ ì„  ë‚œì ìˆ˜', 'í˜¼í•©ëœ ë‚œì ìˆ˜', 'íŒŒíŠ¸ë„ˆ ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜', 'ê¸°ì¦ì ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜', 'ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€', 'ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€', 'ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€', 'ëŒ€ë¦¬ëª¨ ì—¬ë¶€', 'PGD ì‹œìˆ  ì—¬ë¶€', 'PGS ì‹œìˆ  ì—¬ë¶€', 'ë‚œì í˜¼í•© ê²½ê³¼ì¼', 'ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼', 'ë°°ì•„ í•´ë™ ê²½ê³¼ì¼', 'ì‹œìˆ _ì„ì‹ ', 'ë°°ì•„ìƒì„±ì´ìœ _ê¸°ì¦ìš©', 'ë°°ì•„ìƒì„±ì´ìœ _ë‚œì ì €ì¥ìš©', 'ë°°ì•„ìƒì„±ì´ìœ _ë°°ì•„ ì €ì¥ìš©', 'ë°°ì•„ìƒì„±ì´ìœ _í˜„ì¬ ì‹œìˆ ìš©']\n",
      "ë²”ì£¼í˜• ë³€ìˆ˜: 8ê°œ \n",
      "['ì‹œìˆ  ì‹œê¸° ì½”ë“œ', 'ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´', 'ë°°ë€ ìœ ë„ ìœ í˜•', 'ë‚œì ì¶œì²˜', 'ì •ì ì¶œì²˜', 'ë‚œì ê¸°ì¦ì ë‚˜ì´', 'ì •ì ê¸°ì¦ì ë‚˜ì´', 'ì‹œìˆ ìœ í˜•_í†µí•©']\n",
      "(205080, 66) (51271, 65)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7175bdaadb738a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:54:54.875060Z",
     "start_time": "2025-04-03T23:54:54.863060Z"
    }
   },
   "source": [
    "def df_to_tensor(train, val, test=None):\n",
    "    if test is None:\n",
    "        data_numpy = {\n",
    "            'train': {'x_cat': train[cat_cols].values, 'x_cont': train[numeric_cols].values, 'y': train['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].values},\n",
    "            'val': {'x_cat': val[cat_cols].values, 'x_cont': val[numeric_cols].values, 'y': val['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].values},\n",
    "        }\n",
    "        data = {\n",
    "            part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
    "            for part in data_numpy\n",
    "        }\n",
    "        data['train']['y'] = data['train']['y'].long()\n",
    "        data['val']['y'] = data['val']['y'].long()\n",
    "        return data\n",
    "    else:\n",
    "        data_numpy = {\n",
    "            'train': {'x_cat': train[cat_cols].values, 'x_cont': train[numeric_cols].values, 'y': train['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].values},\n",
    "            'val': {'x_cat': val[cat_cols].values, 'x_cont': val[numeric_cols].values, 'y': val['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].values},\n",
    "            'test': {'x_cat': test[cat_cols].values, 'x_cont': test[numeric_cols].values},\n",
    "        }\n",
    "        data = {\n",
    "            part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
    "            for part in data_numpy\n",
    "        }\n",
    "        data['train']['y'] = data['train']['y'].long()\n",
    "        data['val']['y'] = data['val']['y'].long()\n",
    "        return data\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "ff599c4b9c777fd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:55:03.528249Z",
     "start_time": "2025-04-03T23:55:03.312280Z"
    }
   },
   "source": [
    "def get_feature_info(train):\n",
    "    n_num_features_ = len(numeric_cols)\n",
    "    cat_cardinalities_ = [train[col].nunique() for col in cat_cols]\n",
    "\n",
    "    return n_num_features_, cat_cardinalities_\n",
    "\n",
    "n_num_features, cat_cardinalities = get_feature_info(train)\n",
    "bins = rtdl_num_embeddings.compute_bins(torch.tensor(train[numeric_cols].values))\n",
    "\n",
    "model_config = {\n",
    "    'n_num_features': n_num_features,\n",
    "    'cat_cardinalities': cat_cardinalities,\n",
    "    'n_classes': 2,\n",
    "    'backbone': {\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    'bins': bins,\n",
    "    'num_embeddings': (\n",
    "        None if bins is None else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    'arch_type': 'tabm-mini',\n",
    "    'k': 32,\n",
    "    'share_training_batches': True,\n",
    "}\n",
    "\n",
    "model = Model(**model_config).to(device)\n",
    "\n",
    "print(n_num_features)\n",
    "print(cat_cardinalities)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "[7, 7, 2, 3, 4, 5, 7, 9]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "32f926927cf34e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:55:14.556878Z",
     "start_time": "2025-04-03T23:55:13.699035Z"
    }
   },
   "source": [
    "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
    "\n",
    "# ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (ì—¬ê¸°ì„œëŠ” ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ cross_entropy)\n",
    "base_loss_fn = F.cross_entropy\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    # ëª¨ë¸ ì¶œë ¥ y_pred.shape: (batch_size, k, n_classes)\n",
    "    k = y_pred.shape[-2]  # k ì˜ˆì¸¡ ê°œìˆ˜\n",
    "    # ëª¨ë“  ì˜ˆì¸¡ì— ëŒ€í•´ ì†ì‹¤ì„ ê³„ì‚°\n",
    "    return base_loss_fn(\n",
    "        y_pred.flatten(0, 1),\n",
    "        y_true.repeat_interleave(k) if model.share_training_batches else y_true\n",
    "    )\n",
    "\n",
    "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_cont'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "def evaluate(part: str, metric: str = 'auc') -> float:\n",
    "    model.eval()\n",
    "    eval_batch_size = 2048\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for idx in torch.arange(len(data[part]['y']), device=device).split(eval_batch_size):\n",
    "            preds = apply_model(part, idx)\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(data[part]['y'][idx])\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼: (N, k, n_classes) -> (N, n_classes) (kê°œì˜ ì˜ˆì¸¡ì˜ í‰ê· )\n",
    "    y_pred = torch.cat(all_preds).cpu().numpy()\n",
    "    y_true = torch.cat(all_targets).cpu().numpy()\n",
    "\n",
    "    # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ í†µí•´ í™•ë¥ ë¡œ ë³€í™˜í•œ ë’¤ kê°œ ì˜ˆì¸¡ì— ëŒ€í•´ í‰ê· ì„ ëƒ…ë‹ˆë‹¤.\n",
    "    y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
    "    y_pred = y_pred.mean(1)  # shape: (N, n_classes)\n",
    "\n",
    "    if metric == 'auc':\n",
    "        auc = roc_auc_score(y_true, y_pred[:, 1])\n",
    "        return float(auc)\n",
    "    elif metric == 'loss':\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for idx in torch.arange(len(data[part]['y']), device=device).split(eval_batch_size):\n",
    "                preds = apply_model(part, idx)\n",
    "                batch_loss = loss_fn(preds, data[part]['y'][idx])\n",
    "                total_loss += batch_loss.item() * idx.size(0)\n",
    "                total_samples += idx.size(0)\n",
    "        return total_loss / total_samples\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "\n",
    "def predict_proba(part: str) -> np.ndarray:\n",
    "    model.eval()\n",
    "    eval_batch_size = 2048\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        num_samples = len(data[part]['x_cont'])\n",
    "        for idx in torch.arange(num_samples, device=device).split(eval_batch_size):\n",
    "            batch_pred = apply_model(part, idx)\n",
    "            preds.append(batch_pred)\n",
    "\n",
    "    # ëª¨ë“  ë°°ì¹˜ì˜ ì˜ˆì¸¡ê°’ì„ í•˜ë‚˜ì˜ í…ì„œë¡œ í•©ì¹©ë‹ˆë‹¤.\n",
    "    y_pred = torch.cat(preds).cpu().numpy()\n",
    "    y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
    "\n",
    "    return y_pred.mean(1)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:55:24.331277Z",
     "start_time": "2025-04-03T23:55:24.316843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. ìë™ í˜¼í•© ì •ë°€ë„(AMP)ì—ì„œ ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…(amp_dtype)ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "if torch.cuda.is_available():\n",
    "    # CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©´, bfloat16 ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        amp_dtype = torch.bfloat16  # bfloat16ì„ ì§€ì›í•˜ë©´ bfloat16ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    else:\n",
    "        amp_dtype = torch.float16   # bfloat16ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ float16ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "else:\n",
    "    amp_dtype = None  # CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©´ AMPë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "# 2. AMP ê¸°ëŠ¥ì„ í™œì„±í™”í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "amp_enabled = False and (amp_dtype is not None)\n",
    "\n",
    "# 3. GradScalerëŠ” float16 ëª¨ë“œì—ì„œ ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "# amp_dtypeì´ torch.float16ì¼ ë•Œë§Œ GradScalerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "if amp_dtype == torch.float16:\n",
    "    grad_scaler = torch.cuda.amp.GradScaler()\n",
    "else:\n",
    "    grad_scaler = None\n",
    "\n",
    "print(grad_scaler)"
   ],
   "id": "e29f831df3bcd7eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e18ee9209a5acd26",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-03T23:55:40.377349Z"
    }
   },
   "source": [
    "seed = 333\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "train = pd.read_csv(train_path).drop(columns=['ID'])\n",
    "test = pd.read_csv(test_path).drop(columns=['ID'])\n",
    "\n",
    "test_preds = []\n",
    "val_scores = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])):\n",
    "    fold_train = train.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid = train.iloc[valid_idx].copy().reset_index(drop=True)\n",
    "    fold_train2 = fold_train.copy()\n",
    "    fold_test = test.copy()\n",
    "\n",
    "    fold_train, fold_valid = all_process(fold_train, fold_valid)\n",
    "    _, fold_test = all_process(fold_train2, fold_test)\n",
    "\n",
    "    cat_cols = [col for col in fold_train.columns if pd.api.types.is_categorical_dtype(fold_train[col])]\n",
    "    numeric_cols = [col for col in fold_train.columns if col not in cat_cols and col != 'ì„ì‹  ì„±ê³µ ì—¬ë¶€']\n",
    "\n",
    "    bins = rtdl_num_embeddings.compute_bins(torch.tensor(fold_train[numeric_cols].values))\n",
    "    n_num_features, cat_cardinalities = get_feature_info(fold_train)\n",
    "\n",
    "    model_config = {\n",
    "        'n_num_features': n_num_features,\n",
    "        'cat_cardinalities': cat_cardinalities,\n",
    "        'n_classes': 2,\n",
    "        'backbone': {\n",
    "            'type': 'MLP',\n",
    "            'n_blocks': 3 if bins is None else 2,\n",
    "            'd_block': 512,\n",
    "            'dropout': 0.1,\n",
    "        },\n",
    "        'bins': bins,\n",
    "        'num_embeddings': (\n",
    "            None if bins is None else {\n",
    "                'type': 'PiecewiseLinearEmbeddings',\n",
    "                'd_embedding': 16,\n",
    "                'activation': False,\n",
    "                'version': 'B',\n",
    "            }\n",
    "        ),\n",
    "        'arch_type': 'tabm-mini',\n",
    "        'k': 32,\n",
    "        'share_training_batches': True,\n",
    "    }\n",
    "    model = Model(**model_config).to(device)\n",
    "    optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=2e-3, weight_decay=3e-4)\n",
    "\n",
    "    data = df_to_tensor(fold_train, fold_valid, fold_test)\n",
    "\n",
    "    n_epochs = 1000000\n",
    "    patience = 16\n",
    "    early_stopping = 'loss'\n",
    "\n",
    "    train_size = len(fold_train)\n",
    "    batch_size = 1024\n",
    "    epoch_size = math.ceil(train_size / batch_size)\n",
    "    if early_stopping == 'loss':\n",
    "        best = {'val': float('inf'), 'test': float('inf'), 'epoch': -1}\n",
    "    else:  # 'auc'ì¸ ê²½ìš°, ê°’ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ\n",
    "        best = {'val': -float('inf'), 'test': -float('inf'), 'epoch': -1}\n",
    "\n",
    "    patience = 3\n",
    "    remaining_patience = patience\n",
    "\n",
    "    print('-' * 88 + '\\n')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss_epoch = 0.0  # ì „ì²´ í•™ìŠµ ì†ì‹¤ ëˆ„ì \n",
    "        batches = (\n",
    "            torch.randperm(train_size, device=device).split(batch_size)\n",
    "            if model.share_training_batches\n",
    "            else [\n",
    "                x.transpose(0, 1).flatten()\n",
    "                for x in torch.rand((model.k, train_size), device=device)\n",
    "                .argsort(dim=1)\n",
    "                .split(batch_size, dim=1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # for batch_idx in tqdm(batches, desc=f'Epoch {epoch}'):\n",
    "        for batch_idx in batches:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(apply_model('train', batch_idx), data['train']['y'][batch_idx])\n",
    "            train_loss_epoch += loss.item() * batch_idx.size(0)\n",
    "            if grad_scaler is None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                grad_scaler.scale(loss).backward()  # type: ignore\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "        train_loss_epoch /= train_size\n",
    "\n",
    "        if early_stopping == 'loss':\n",
    "            val_metric = evaluate('val', metric='loss')\n",
    "            val_auc = evaluate('val', metric='auc')\n",
    "            y_test_pred = predict_proba('test')[:, 1]\n",
    "            test_metric = calculate_auc(y_test_pred, data_seed)\n",
    "        elif early_stopping == 'auc':\n",
    "            val_metric = evaluate('val', metric='auc')\n",
    "            y_test_pred = predict_proba('test')[:, 1]\n",
    "            test_metric = calculate_auc(y_test_pred, data_seed)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {early_stopping}\")\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss_epoch:.4f}, Val {early_stopping.capitalize()}: {val_metric:.4f},\"\n",
    "              f\"Val AUC: {val_auc}, Test AUC: {test_metric:.4f}\")\n",
    "\n",
    "        if early_stopping == 'loss':\n",
    "            if val_metric < best['val']:\n",
    "                # print(\"ğŸŒ¸ New best epoch! ğŸŒ¸\")\n",
    "                best = {'val': val_metric, 'test': test_metric, 'epoch': epoch}\n",
    "                torch.save(model.state_dict(), \"best_model.pt\")\n",
    "                remaining_patience = patience\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "\n",
    "        elif early_stopping == 'auc':\n",
    "            if val_metric > best['val']:\n",
    "                # print(\"ğŸŒ¸ New best epoch! ğŸŒ¸\")\n",
    "                best = {'val': val_metric, 'test': test_metric, 'epoch': epoch}\n",
    "                torch.save(model.state_dict(), \"best_model.pt\")\n",
    "                remaining_patience = patience\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "\n",
    "        if remaining_patience < 0:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    print('\\n\\nResult:')\n",
    "    print(best)\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    print(\"Best model weights loaded from best_model.pt\")\n",
    "\n",
    "    val_score = evaluate('val', metric='auc')\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "    y_test_pred = predict_proba('test')[:, 1]\n",
    "    test_preds.append(y_test_pred)\n",
    "\n",
    "valid_auc = np.mean(val_scores, axis=0)\n",
    "\n",
    "test_auc = calculate_auc(np.mean(test_preds, axis=0), data_seed)\n",
    "\n",
    "print(f'[Data Seed {data_seed}] Validation AUC: {valid_auc:.4f}, Test AUC: {test_auc}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch000 - Train Loss: 0.5103, Val Loss: 0.4932, Test AUC: 0.7365\n",
      "\n",
      "Epoch001 - Train Loss: 0.4927, Val Loss: 0.4912, Test AUC: 0.7377\n",
      "\n",
      "Epoch002 - Train Loss: 0.4909, Val Loss: 0.4907, Test AUC: 0.7387\n",
      "\n",
      "Epoch003 - Train Loss: 0.4897, Val Loss: 0.4917, Test AUC: 0.7385\n",
      "\n",
      "Epoch004 - Train Loss: 0.4892, Val Loss: 0.4902, Test AUC: 0.7394\n",
      "\n",
      "Epoch005 - Train Loss: 0.4887, Val Loss: 0.4894, Test AUC: 0.7393\n",
      "\n",
      "Epoch006 - Train Loss: 0.4882, Val Loss: 0.4911, Test AUC: 0.7391\n",
      "\n",
      "Epoch007 - Train Loss: 0.4878, Val Loss: 0.4914, Test AUC: 0.7395\n",
      "\n",
      "Epoch008 - Train Loss: 0.4876, Val Loss: 0.4914, Test AUC: 0.7395\n",
      "\n",
      "Epoch009 - Train Loss: 0.4872, Val Loss: 0.4926, Test AUC: 0.7396\n",
      "Early stopping triggered.\n",
      "\n",
      "\n",
      "Result:\n",
      "{'val': 0.48943414839476157, 'test': 0.7392973721465341, 'epoch': 5}\n",
      "Best model weights loaded from best_model.pt\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch000 - Train Loss: 0.5118, Val Loss: 0.4906, Test AUC: 0.7350\n",
      "\n",
      "Epoch001 - Train Loss: 0.4924, Val Loss: 0.4880, Test AUC: 0.7375\n",
      "\n",
      "Epoch002 - Train Loss: 0.4910, Val Loss: 0.4880, Test AUC: 0.7388\n",
      "\n",
      "Epoch003 - Train Loss: 0.4899, Val Loss: 0.4874, Test AUC: 0.7387\n",
      "\n",
      "Epoch004 - Train Loss: 0.4896, Val Loss: 0.4873, Test AUC: 0.7388\n",
      "\n",
      "Epoch005 - Train Loss: 0.4892, Val Loss: 0.4865, Test AUC: 0.7392\n",
      "\n",
      "Epoch006 - Train Loss: 0.4886, Val Loss: 0.4868, Test AUC: 0.7388\n",
      "\n",
      "Epoch007 - Train Loss: 0.4883, Val Loss: 0.4903, Test AUC: 0.7396\n",
      "\n",
      "Epoch008 - Train Loss: 0.4879, Val Loss: 0.4866, Test AUC: 0.7393\n",
      "\n",
      "Epoch009 - Train Loss: 0.4875, Val Loss: 0.4882, Test AUC: 0.7399\n",
      "Early stopping triggered.\n",
      "\n",
      "\n",
      "Result:\n",
      "{'val': 0.48646371080230405, 'test': 0.7392243772647726, 'epoch': 5}\n",
      "Best model weights loaded from best_model.pt\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch000 - Train Loss: 0.5095, Val Loss: 0.4944, Test AUC: 0.7363\n",
      "\n",
      "Epoch001 - Train Loss: 0.4925, Val Loss: 0.5024, Test AUC: 0.7376\n",
      "\n",
      "Epoch002 - Train Loss: 0.4909, Val Loss: 0.4910, Test AUC: 0.7363\n",
      "\n",
      "Epoch003 - Train Loss: 0.4900, Val Loss: 0.4877, Test AUC: 0.7390\n",
      "\n",
      "Epoch004 - Train Loss: 0.4894, Val Loss: 0.4892, Test AUC: 0.7389\n",
      "\n",
      "Epoch005 - Train Loss: 0.4893, Val Loss: 0.4910, Test AUC: 0.7390\n",
      "\n",
      "Epoch006 - Train Loss: 0.4887, Val Loss: 0.4883, Test AUC: 0.7394\n",
      "\n",
      "Epoch007 - Train Loss: 0.4880, Val Loss: 0.4880, Test AUC: 0.7394\n",
      "Early stopping triggered.\n",
      "\n",
      "\n",
      "Result:\n",
      "{'val': 0.48765941552188585, 'test': 0.7390098676905725, 'epoch': 3}\n",
      "Best model weights loaded from best_model.pt\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch000 - Train Loss: 0.5100, Val Loss: 0.4979, Test AUC: 0.7351\n",
      "\n",
      "Epoch001 - Train Loss: 0.4922, Val Loss: 0.4957, Test AUC: 0.7379\n",
      "\n",
      "Epoch002 - Train Loss: 0.4907, Val Loss: 0.4913, Test AUC: 0.7381\n",
      "\n",
      "Epoch003 - Train Loss: 0.4896, Val Loss: 0.4913, Test AUC: 0.7381\n",
      "\n",
      "Epoch004 - Train Loss: 0.4892, Val Loss: 0.4913, Test AUC: 0.7392\n",
      "\n",
      "Epoch005 - Train Loss: 0.4882, Val Loss: 0.4911, Test AUC: 0.7394\n",
      "\n",
      "Epoch006 - Train Loss: 0.4880, Val Loss: 0.4906, Test AUC: 0.7396\n",
      "\n",
      "Epoch007 - Train Loss: 0.4878, Val Loss: 0.4919, Test AUC: 0.7395\n",
      "\n",
      "Epoch008 - Train Loss: 0.4872, Val Loss: 0.4918, Test AUC: 0.7391\n",
      "\n",
      "Epoch009 - Train Loss: 0.4869, Val Loss: 0.4910, Test AUC: 0.7394\n",
      "\n",
      "Epoch010 - Train Loss: 0.4868, Val Loss: 0.4908, Test AUC: 0.7383\n",
      "Early stopping triggered.\n",
      "\n",
      "\n",
      "Result:\n",
      "{'val': 0.4905514073506745, 'test': 0.7395818677425897, 'epoch': 6}\n",
      "Best model weights loaded from best_model.pt\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch000 - Train Loss: 0.5106, Val Loss: 0.4907, Test AUC: 0.7355\n",
      "\n",
      "Epoch001 - Train Loss: 0.4926, Val Loss: 0.4868, Test AUC: 0.7380\n",
      "\n",
      "Epoch002 - Train Loss: 0.4909, Val Loss: 0.4879, Test AUC: 0.7385\n",
      "\n",
      "Epoch003 - Train Loss: 0.4902, Val Loss: 0.4867, Test AUC: 0.7391\n",
      "\n",
      "Epoch004 - Train Loss: 0.4899, Val Loss: 0.4890, Test AUC: 0.7398\n",
      "\n",
      "Epoch005 - Train Loss: 0.4889, Val Loss: 0.4866, Test AUC: 0.7392\n",
      "\n",
      "Epoch006 - Train Loss: 0.4884, Val Loss: 0.4854, Test AUC: 0.7400\n",
      "\n",
      "Epoch007 - Train Loss: 0.4886, Val Loss: 0.4859, Test AUC: 0.7394\n",
      "\n",
      "Epoch008 - Train Loss: 0.4878, Val Loss: 0.4872, Test AUC: 0.7387\n",
      "\n",
      "Epoch009 - Train Loss: 0.4876, Val Loss: 0.4864, Test AUC: 0.7392\n",
      "\n",
      "Epoch010 - Train Loss: 0.4871, Val Loss: 0.4862, Test AUC: 0.7395\n",
      "Early stopping triggered.\n",
      "\n",
      "\n",
      "Result:\n",
      "{'val': 0.4853698376828475, 'test': 0.7400052245561485, 'epoch': 6}\n",
      "Best model weights loaded from best_model.pt\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch000 - Train Loss: 0.5103, Val Loss: 0.4963, Test AUC: 0.7355\n",
      "\n",
      "Epoch001 - Train Loss: 0.4919, Val Loss: 0.4944, Test AUC: 0.7378\n",
      "\n",
      "Epoch002 - Train Loss: 0.4904, Val Loss: 0.4927, Test AUC: 0.7386\n",
      "\n",
      "Epoch003 - Train Loss: 0.4893, Val Loss: 0.4945, Test AUC: 0.7393\n",
      "\n",
      "Epoch004 - Train Loss: 0.4887, Val Loss: 0.4936, Test AUC: 0.7392\n",
      "\n",
      "Epoch005 - Train Loss: 0.4885, Val Loss: 0.4925, Test AUC: 0.7393\n",
      "\n",
      "Epoch006 - Train Loss: 0.4880, Val Loss: 0.4932, Test AUC: 0.7392\n",
      "\n",
      "Epoch007 - Train Loss: 0.4875, Val Loss: 0.4930, Test AUC: 0.7394\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2b7f75-b22e-465f-a646-ce56d96303a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:15:51.996208Z",
     "start_time": "2025-03-30T02:15:51.986207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tabm_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.446093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51266</th>\n",
       "      <td>0.231313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51267</th>\n",
       "      <td>0.255745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51268</th>\n",
       "      <td>0.141795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51269</th>\n",
       "      <td>0.029850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51270</th>\n",
       "      <td>0.151660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51271 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tabm_10\n",
       "0      0.341478\n",
       "1      0.105108\n",
       "2      0.000232\n",
       "3      0.114351\n",
       "4      0.446093\n",
       "...         ...\n",
       "51266  0.231313\n",
       "51267  0.255745\n",
       "51268  0.141795\n",
       "51269  0.029850\n",
       "51270  0.151660\n",
       "\n",
       "[51271 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({f'tabm_{data_seed}': np.mean(test_preds, axis=0)})\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64672a7154d813d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:15:53.055439Z",
     "start_time": "2025-03-30T02:15:52.091208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[seed 10]: 0.74150757679765\n"
     ]
    }
   ],
   "source": "submission.to_csv(f'TabM_{data_seed}.csv', index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189382d5401e8bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:15:53.117442Z",
     "start_time": "2025-03-30T02:15:53.103441Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
